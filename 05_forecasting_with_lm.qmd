---
title: "Forecasting with Linear Regression"
author: "Rami Krispin"
date: last-modified
format: 
    html:
        code-fold: false
        warning: false
        toc: true
---




## Required Libraries
```{r}
library(feasts)
library(fabletools)
library(tsibble)
library(dplyr)
library(plotly)
```

Loading supporting functions:

```{r}
source("./functions.R")
```


## Load Data


```{r}
load(file = "./data/ts.RData")
```


## Modeling Trend

```{r}
head(ts1)

ts1 <- ts1 |> 
dplyr::filter(index > 1986)


p <- plot_ly() |>
add_lines(x = ts1$index, y = ts1$y, type = "scatter", mode = "lines", name = "Actual") 

p
```


### Fitting a Linear Trend

To model the trend we will create an index variable and use linear regression model fit the index against the dependent variable - the series of interest:

```{r}
ts1 <- ts1 |> 
dplyr::mutate(trend = 1:nrow(ts1))

head(ts1)
```


Let's now set a regression model:

```{r}
md1 <- lm(y ~ trend, data = ts1)

summary(md1)
```

We will fit the model on the series to see the trend fit:

```{r}
fit1 <- predict(object = md1, newdata = ts1,  interval = "confidence", level = 0.95)
ts1$fit1 <- fit1[, 1]
p1 <- p |> 
add_lines(x = ts1$index, y = ts1$fit1, mode = "lines", line = list(color = "black", dash = "dash"), name = "Fitted") 

p1
```


### Residuals Analysis

The goal of the residual analysis is to check the goodness of fit of the model with data the it was encountered during the fit process. The residual could indicate whether the model has a good fit or or some patterns left and a direction about the type of features that are needed for better predictions.

```{r}
ts1$res1 <- ts1$y - ts1$fit1


plot_ly(x = ts1$index, y = ts1$res1, type = "scatter", mode = "markers")
```

Clearly, you can see that some patterns are left in the residuals. We can check for autocorrelation:

```{r}
plot_acf(ts = ts1, var = "res1", lag_max = 60, frequency = NULL, alpha = 0.05)
```

The ACF plot indicates that the residuals have a high correlation with the first lags. This is classic case, that adding an ARIMA model (i.e., regression with ARIMA errors) could improve the fit of the model.

### Forecast the Trend

To forecast the future observations of the series, we will have to generate a new data frame with the corresponding features we used to train the model. Let' say that we want to generate a 5 year forecast. We will use the `new_data` function from the **tsibble** library to generate a new data frame with the corresponding features for the next 5 years:

```{r}
h <- 5
future_data <- new_data(ts1, n = h)
future_data
```

Next, we will generate the trend feature:

```{r}
trend_start <- max(ts1$trend) + 1
trend_end <- trend_start + h - 1
future_data$trend <- trend_start:trend_end
future_data
```

Now we can use the `predict` function to create the forecast:

```{r}
fc1 <- predict(object = md1, newdata = future_data, interval = "prediction", level = 0.95)

future_data$yhat <- fc1[,1]
future_data$lower <- fc1[,2]
future_data$upper <- fc1[,3]
future_data
```

Let's plot the forecast
```{r}
p1 |> 
add_ribbons(x = future_data$index, ymin = future_data$lower,
ymax = future_data$upper, name = "95% Prediction Interval",
 line = list(color = 'rgba(7, 164, 181, 0.05)'),
              fillcolor = 'rgba(7, 164, 181, 0.2)') |>
add_lines(x = future_data$index, 
y = future_data$yhat, 
name = "Forecast", 
line = list(color = "black", dash = "dash"))
```

### Improving the model

Reviewing the results, there are two features we can use to improve the forecast:

- Piecewise linear trend
- Adding AR model 
- Combing both approaches


#### Piecewise Linear Trend

Eyeballing the series, you can notice a change in the trend at around 2008. We will use this information to create a piecewise linear trend.

```{r}
s <- ts1$trend[which(ts1$index == 2008)]
ts1$trend2 <- pmax(0, ts1$trend - s)

md2 <- lm(y ~ trend + trend2, data = ts1)

summary(md2)

```


Let's fit the model and plot it:

```{r}
fit2<- predict(object = md2, newdata = ts1,  interval = "confidence", level = 0.95)
ts1$fit2 <- fit2[, 1]
p2 <- p |> 
add_lines(x = ts1$index, y = ts1$fit2, mode = "lines", line = list(color = "black", dash = "dash"), name = "Fitted") 
p2
```

We will add to the plot the features:

```{r}
trend1 <- plot_ly() |>
add_lines(x = ts1$index, y = ts1$trend, name = "Main Trend Feature")

trend2 <- plot_ly() |>
add_lines(x = ts1$index, y = ts1$trend2, name = "Piecewise Trend Feature")
```


```{r}
#| fig-height: 10
subplot(p2, trend1, trend2, nrows = 3)

```


Let's review the residuals:

```{r}
ts1$res2 <- ts1$y - ts1$fit2


plot_ly(x = ts1$index, y = ts1$res2, type = "scatter", mode = "markers")
```


And the ACF of the residuals:
```{r}
plot_acf(ts = ts1, var = "res2", lag_max = 60, frequency = NULL, alpha = 0.05)
```

We can noticed from the residuals and ACF plots that the model goodness of fit improved with the addition of a second trend feature. Yet, the ACF plot indicate that some correlation left on the residuals within the first lag. An AR model with order 1 can help to remove this correlation.

### AR model

The Auto Regressive (AR) model is a common approach in time series forecasting to model time series with high correlation with its past lags. The AR model is part of a family of models that construct the ARIMA family of models.

At its core, AR model is a regression of the series with its previous lags. For example, AR 1 model would be a regression of the series with its lag one:

$$
Y_t = \alpha + \beta * Y_{t-1} 
$$

Let's create a lag 1 feature:

```{r}
ts1$lag1 <- dplyr::lag(ts1$y, n = 1)

ts1
```


Let's now fit the model:

```{r}
md3 <- lm(y ~ trend + lag1, data = ts1)

summary(md3)

```

We will refit the model and plot it:
```{r}
fit3<- predict(object = md3, newdata = ts1,  interval = "confidence", level = 0.95)
ts1$fit3 <- fit3[, 1]
p3 <- p |> 
add_lines(x = ts1$index, y = ts1$fit3, mode = "lines", line = list(color = "black", dash = "dash"), name = "Fitted") 
```

Let's review the residual ACF plot:

```{r}
ts1$res3 <- ts1$y - ts1$fit3

plot_acf(ts = ts1, var = "res3", lag_max = 60, frequency = NULL, alpha = 0.05)
```



::: {.callout-warning}
## Using Lags

The main downside of using lags are that you will have to produce the lags for your forecast assuming the lag number is smaller than the forecast horizon. In this case, you will have to use a recursive function and use previous predication values as lags for your next prediction.
:::

A great article about piecewise linear trends modeling can be found in the following [blog post](https://robjhyndman.com/hyndsight/piecewise-linear-trends/) by Prof. Rob J Hyndman.



## Modeling Seasonal Time Series

Let's now see how can we model a seasonal time series with linear regression.

```{r}
p <- plot_ly() |>
add_lines(x = ts2$date, y = ts2$y, name = "Actual")

p
```


Let's decompose the series:

```{r}
stl_d <- ts2 |> model( STL(y)) |> components()

plot_decomposition(stl_d, var = "y", outliers = TRUE)
```


We will build the features step-by-step to review the impact of each feature on the goodness of fit. Let's start by defining the trend feature:

```{r}
ts2$trend <- 1:nrow(ts2)

ts2
```

```{r}
mds1  <- lm(y ~ trend, data = ts2)

summary(mds1)
```

Let's fit the model and plot it:

```{r}
fit_s1 <- predict(object = mds1, newdata = ts2,  interval = "confidence", level = 0.95)
ts2$fit1 <- fit_s1[, 1]
p1 <- p |> 
add_lines(x = ts2$date, y = ts2$fit1, mode = "lines", line = list(color = "black", dash = "dash"), name = "Fitted") 

p1
```

```{r}
ts2$res1 <- ts2$y - ts2$fit1


```

::: {.callout-tip}
## Detranding the Series

If you check the model residuals, you will noticed that the residuals are detrained version of the original series.
:::



We will now add the seasonal feature by creating a categorical variable for each month of the year:

```{r}
ts2$month <- lubridate::month(ts2$index, label = TRUE)
ts2
```


```{r}
mds2  <- lm(y ~ trend + month, data = ts2)

summary(mds2)
```

As you can  noticed from the model summary, adding the seasonal feature improved the model goodness of fit.


```{r}
fit_s2 <- predict(object = mds2, newdata = ts2,  interval = "confidence", level = 0.95)
ts2$fit2 <- fit_s2[, 1]
p2 <- p |> 
add_lines(x = ts2$date, y = ts2$fit2, mode = "lines", line = list(color = "black", dash = "dash"), name = "Fitted") 

p2
```

Let's review now the model residuals:

```{r}
ts2$res2 <- ts2$y - ts2$fit2


plot_ly(x = ts2$date, y = ts2$res2, type = "scatter", mode = "markers")
```


And the ACF of the residuals:
```{r}
plot_acf(ts = ts2, var = "res2", lag_max = 60, frequency = NULL, alpha = 0.05)
```



The is a ton of information that left on the residuals!

We will explore the following features:

- Structural breaks
- Piecewise Linear Trend
- AR
- Outliers

### Structural Break Feature

Often, time series would have an abrupt shift in its structure due to some event. A good example, is the Covid-19 pandemic impact on some macro-economic indicators such as the unemployment rate and number of passengers traveling on airplanes. At the end of this change, the series would be in a new level. 

We can notice that the natural gas series had a structural break on the trend component around September 2018. To model a structural break, we will create a new variable that is equal to 0 prior to September 2018 and 1 after this date:

```{r}
ts2$structural_break <- ifelse(ts2$date >= as.Date("2018-09-01"), 1, 0)

ts2 |> head()

ts2 |> tail()
```




```{r}
mds3  <- lm(y ~ trend + month + structural_break, data = ts2)

summary(mds3)
```

As you can  noticed from the model summary, adding the seasonal feature improved the model goodness of fit.


```{r}
fit_s3 <- predict(object = mds3, newdata = ts2,  interval = "confidence", level = 0.95)
ts2$fit3 <- fit_s3[, 1]
p3 <- p |> 
add_lines(x = ts2$date, y = ts2$fit3, mode = "lines", line = list(color = "black", dash = "dash"), name = "Fitted") 

p3
```


let's plot the residuals:

```{r}
ts2$res3 <- ts2$y - ts2$fit3


plot_ly(x = ts2$date, y = ts2$res3, type = "scatter", mode = "markers")
```


Let's generate a forecast for the next 5 years (60 months):


```{r}
h <- 60
future_data <- new_data(ts2, n = h)
future_data
```
We will have to build all the features we used so far: 

```{r}
trend_start <- max(ts2$trend) + 1
trend_end <- trend_start + h - 1
future_data$trend <- trend_start:trend_end
future_data$month <- lubridate::month(future_data$index, label = TRUE)
future_data$structural_break <- 1
future_data$date <- as.Date(future_data$index)
future_data
```

Now we can use the `predict` function to create the forecast:

```{r}
fcs1 <- predict(object = mds3, newdata = future_data, interval = "prediction", level = 0.95)

future_data$yhat <- fcs1[,1]
future_data$lower <- fcs1[,2]
future_data$upper <- fcs1[,3]
future_data
```

Let's plot the forecast
```{r}
p3 |> 
add_ribbons(x = future_data$date, ymin = future_data$lower,
ymax = future_data$upper, name = "95% Prediction Interval",
 line = list(color = 'rgba(7, 164, 181, 0.05)'),
              fillcolor = 'rgba(7, 164, 181, 0.2)') |>
add_lines(x = future_data$date, 
y = future_data$yhat, 
name = "Forecast", 
line = list(color = "black", dash = "dash"))
```


### Handling Outliers

We can notices that the series has some years where the consumption is higher than normal years. We can segment two groups of outliers:

- High: Jan 2025
- Medium: Jan 2014, Jan 2018, Jan 2019, Jan 2022, Jan 2024

Let's create the features:

```{r}
ts2$outlier_high <- ifelse(ts2$date == as.Date("2025-01-01"), 1, 0)
outlier_medium_dates <-  c(as.Date("2014-01-01"), as.Date("2018-01-01"),as.Date("2019-01-01"), as.Date("2022-01-01"),as.Date("2024-01-01"))
ts2$outlier_medium <- ifelse(ts2$date %in% outlier_medium_dates, 1, 0)

table(ts2$outlier_high)
table(ts2$outlier_medium)
```

Let's train the model with the new features:

```{r}
mds4  <- lm(y ~ trend + month + structural_break + outlier_high + outlier_medium, data = ts2)

summary(mds4)
```

Let's refit the model and plot the residuals:

```{r}
fit_s4 <- predict(object = mds4, newdata = ts2,  interval = "confidence", level = 0.95)
ts2$fit4 <- fit_s4[, 1]
p4 <- p |> 
add_lines(x = ts2$date, y = ts2$fit4, mode = "lines", line = list(color = "black", dash = "dash"), name = "Fitted") 

p4
```




```{r}
ts2$res4 <- ts2$y - ts2$fit4


plot_ly(x = ts2$date, y = ts2$res4, type = "scatter", mode = "markers")
```

What else can be done?

- Segment the residual with standard deviation
- Add AR model to capture the correlation between residuals

## Simulation

Often, there are unexpected events that we won't know if and when they will occur in the future. We saw in the series that there are years that the consumption during the month of January is significantly higher. One approach to mitigate this risk is to use external information about future behavior such as weather prediction. Alternatively, we can set some assumption and about the probability of those events in the future and conduct simulation. 

In the following example, we will illustrate the impact of training outlier and add its impact to the forecast. Let's first create a baseline forecast:

```{r}
future_data$outlier_high <- 0
future_data$outlier_medium <- 0

fcs2 <- predict(object = mds4, newdata = future_data, interval = "prediction", level = 0.95)

future_data$yhat2 <- fcs2[,1]
future_data$lower2 <- fcs2[,2]
future_data$upper2 <- fcs2[,3]

p4 |> 
add_ribbons(x = future_data$date, ymin = future_data$lower2,
ymax = future_data$upper2, name = "95% Prediction Interval",
 line = list(color = 'rgba(7, 164, 181, 0.05)'),
              fillcolor = 'rgba(7, 164, 181, 0.2)') |>
add_lines(x = future_data$date, 
y = future_data$yhat2, 
name = "Forecast", 
line = list(color = "black", dash = "dash"))

```


Now, let's assume that:
- High consumption is expected in January 2029
- Medium consumption is expected in January 2027

Let's update the features on the future data set:

```{r}
future_data$outlier_high <-ifelse(future_data$date == "2029-01-01", 1, future_data$outlier_high)

future_data$outlier_medium <-ifelse(future_data$date == "2027-01-01", 1, future_data$outlier_medium)

```


```{r}
fcs3 <- predict(object = mds4, newdata = future_data, interval = "prediction", level = 0.95)

future_data$yhat3 <- fcs3[,1]
future_data$lower3 <- fcs3[,2]
future_data$upper3 <- fcs3[,3]

p4 |> 
add_ribbons(x = future_data$date, ymin = future_data$lower3,
ymax = future_data$upper3, name = "95% Prediction Interval",
 line = list(color = 'rgba(7, 164, 181, 0.05)'),
              fillcolor = 'rgba(7, 164, 181, 0.2)') |>
add_lines(x = future_data$date, 
y = future_data$yhat3, 
name = "Forecast", 
line = list(color = "black", dash = "dash"))
```




